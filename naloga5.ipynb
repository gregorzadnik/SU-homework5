{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    data.drop(columns=data.columns[0], axis=1, inplace=True)\n",
    "    data.set_index(\"X\", inplace=True)\n",
    "    data = pd.get_dummies(data,columns=['alcohol_level'], drop_first=True)\n",
    "    data.index.name = None\n",
    "    y = data['quality']\n",
    "    X = data.drop(columns='quality')\n",
    "    y = np.where(y == 'high', 1, 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, epsilon=0.001):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.epsilon = epsilon\n",
    "        self.models = []\n",
    "        self.mse = []\n",
    "\n",
    "    def calc_probability(self, y):\n",
    "        log = np.log(sum(y)/(len(y)-sum(y)))\n",
    "        return np.exp(log)/(1+np.exp(log))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the log(odds)\n",
    "        initial_prediction = self.calc_probability(y)\n",
    "        # Make initial prediction\n",
    "        predictions = np.full_like(y, initial_prediction, dtype=float)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            error = mean_squared_error(y, predictions)\n",
    "            if len(self.mse) > 5 and np.abs(self.mse[len(self.mse)-2]-error) < self.epsilon:\n",
    "                print(f\"Overfitting, stopping the fit at {len(self.mse)} trees!\")\n",
    "                break\n",
    "            self.mse.append(error)\n",
    "            residuals = y - predictions\n",
    "            # Fit a weak learner to the negative gradient (residuals)\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            # Make predictions with the weak learner\n",
    "            weak_learner_predictions = model.predict(X)\n",
    "            # Update the ensemble's predictions with a fraction of the weak learner's predictions\n",
    "            predictions += self.learning_rate * weak_learner_predictions\n",
    "            # Save the weak learner in the ensemble\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # For classification problems, initialize with the log(odds) or probability\n",
    "        #predictions = np.full(X.shape[0], np.mean([model.tree_.value.max() for model in self.models]))\n",
    "        predictions = np.full(X.shape[0], self.calc_probability([model.tree_.value.max() for model in self.models]))\n",
    "        #print(predictions)\n",
    "        # Accumulate predictions from each weak learner\n",
    "        for model in self.models:\n",
    "            weak_learner_predictions = model.predict(X)\n",
    "            predictions += self.learning_rate * weak_learner_predictions\n",
    "        # Convert to binary predictions for classification problems\n",
    "        #print(predictions)\n",
    "        binary_predictions = np.where(predictions >= 0.5, 1, 0)\n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data\n",
    "X, y = prepare_data(\"wine_quality.csv\")\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out different learning rates\n",
      "Overfitting, stopping the fit at 6 trees!\n",
      "Learning rate: 1e-05, Accuracy: 0.7761904761904762\n",
      "Learning rate: 0.0001, Accuracy: 0.7761904761904762\n",
      "Learning rate: 0.001, Accuracy: 0.7761904761904762\n",
      "Learning rate: 0.1, Accuracy: 0.7925170068027211\n",
      "Learning rate: 0.3, Accuracy: 0.7870748299319728\n",
      "Learning rate: 0.5, Accuracy: 0.827891156462585\n",
      "Learning rate: 0.7, Accuracy: 0.8258503401360544\n",
      "Learning rate: 0.85, Accuracy: 0.8054421768707483\n",
      "Learning rate: 1, Accuracy: 0.7843537414965986\n",
      "Learning rate: 10, Accuracy: 0.7761904761904762\n",
      "A good learning rate is 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grza2520\\AppData\\Local\\Temp\\ipykernel_13284\\3720545887.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  log = np.log(sum(y)/(len(y)-sum(y)))\n"
     ]
    }
   ],
   "source": [
    "print(\"Trying out different learning rates\")\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.1, 0.3, 0.5, 0.7, 0.85, 1, 10]\n",
    "for alpha in alphas:\n",
    "    model = MyGradientBoostingClassifier(n_estimators=100, learning_rate=alpha, epsilon=0.00001)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Learning rate: {alpha}, Accuracy: {accuracy}\")\n",
    "    #plt.plot(range(len(model.mse)), model.mse)\n",
    "print(\"A good learning rate is 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different numbers of trees\n",
      "Number of trees: 1, Accuracy: 0.7952380952380952\n",
      "Number of trees: 20, Accuracy: 0.7183673469387755\n",
      "Number of trees: 50, Accuracy: 0.791156462585034\n",
      "Number of trees: 100, Accuracy: 0.827891156462585\n",
      "Number of trees: 200, Accuracy: 0.8517006802721089\n",
      "Number of trees: 400, Accuracy: 0.8612244897959184\n",
      "Overfitting, stopping the fit at 588 trees!\n",
      "Number of trees: 600, Accuracy: 0.8646258503401361\n",
      "Overfitting, stopping the fit at 588 trees!\n",
      "Number of trees: 800, Accuracy: 0.863265306122449\n",
      "Overfitting, stopping the fit at 588 trees!\n",
      "Number of trees: 1000, Accuracy: 0.863265306122449\n",
      "Overfitting, stopping the fit at 588 trees!\n",
      "Number of trees: 1500, Accuracy: 0.8659863945578231\n",
      "Overfitting, stopping the fit at 588 trees!\n",
      "Number of trees: 3000, Accuracy: 0.8659863945578231\n",
      "The model started overfitting at around 700 trees. I prevented it by implementing an epsilon. if the loss function change in two iterations is below this epsilon, the fitting stops.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing different numbers of trees\")\n",
    "alphas = [1, 20, 50, 100, 200, 400, 600, 800, 1000, 1500, 3000]\n",
    "for alpha in alphas:\n",
    "    model = MyGradientBoostingClassifier(n_estimators=alpha, learning_rate=0.5, epsilon=0.00001)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Number of trees: {alpha}, Accuracy: {accuracy}\")\n",
    "    #plt.plot(range(len(model.mse)), model.mse)\n",
    "\n",
    "print(\"The model started overfitting at around 700 trees. I prevented it by implementing an epsilon. if the loss function change in two iterations is below this epsilon, the fitting stops.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing my classifier and scikit-learn's\n",
      "My accuracy: 0.8659863945578231\n",
      "SK accuracy: 0.863265306122449\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing my classifier and scikit-learn's\")\n",
    "my_model = MyGradientBoostingClassifier(n_estimators=400, max_depth=3, learning_rate=0.5, epsilon=0.00001)\n",
    "my_model.fit(X_train, y_train)\n",
    "y_pred = my_model.predict(X_test)\n",
    "my_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"My accuracy: {accuracy}\")\n",
    "sk_gb = GradientBoostingClassifier(n_estimators=400, max_depth=3, learning_rate=0.5)\n",
    "sk_gb.fit(X_train, y_train)\n",
    "y_pred = sk_gb.predict(X_test)\n",
    "sk_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"SK accuracy: {sk_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing XGBoost, LightGBM and CatBoost\n",
      "[LightGBM] [Info] Number of positive: 731, number of negative: 2697\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1230\n",
      "[LightGBM] [Info] Number of data points in the train set: 3428, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.213244 -> initscore=-1.305482\n",
      "[LightGBM] [Info] Start training from score -1.305482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "0:\tlearn: 0.6559831\ttotal: 1.53ms\tremaining: 152ms\n",
      "1:\tlearn: 0.6259296\ttotal: 2.91ms\tremaining: 143ms\n",
      "2:\tlearn: 0.6013461\ttotal: 4.21ms\tremaining: 136ms\n",
      "3:\tlearn: 0.5791206\ttotal: 5.49ms\tremaining: 132ms\n",
      "4:\tlearn: 0.5595701\ttotal: 6.83ms\tremaining: 130ms\n",
      "5:\tlearn: 0.5429802\ttotal: 8.16ms\tremaining: 128ms\n",
      "6:\tlearn: 0.5289734\ttotal: 9.45ms\tremaining: 126ms\n",
      "7:\tlearn: 0.5167539\ttotal: 10.8ms\tremaining: 124ms\n",
      "8:\tlearn: 0.5061933\ttotal: 12.1ms\tremaining: 123ms\n",
      "9:\tlearn: 0.4958458\ttotal: 13.5ms\tremaining: 121ms\n",
      "10:\tlearn: 0.4885356\ttotal: 14.8ms\tremaining: 120ms\n",
      "11:\tlearn: 0.4818772\ttotal: 16.1ms\tremaining: 118ms\n",
      "12:\tlearn: 0.4754332\ttotal: 17.8ms\tremaining: 119ms\n",
      "13:\tlearn: 0.4698364\ttotal: 19.2ms\tremaining: 118ms\n",
      "14:\tlearn: 0.4646270\ttotal: 20.5ms\tremaining: 116ms\n",
      "15:\tlearn: 0.4604828\ttotal: 21.8ms\tremaining: 114ms\n",
      "16:\tlearn: 0.4565993\ttotal: 23ms\tremaining: 112ms\n",
      "17:\tlearn: 0.4524940\ttotal: 24.5ms\tremaining: 111ms\n",
      "18:\tlearn: 0.4489436\ttotal: 25.8ms\tremaining: 110ms\n",
      "19:\tlearn: 0.4457527\ttotal: 27ms\tremaining: 108ms\n",
      "20:\tlearn: 0.4437697\ttotal: 28.3ms\tremaining: 107ms\n",
      "21:\tlearn: 0.4413355\ttotal: 29.6ms\tremaining: 105ms\n",
      "22:\tlearn: 0.4389218\ttotal: 30.8ms\tremaining: 103ms\n",
      "23:\tlearn: 0.4367966\ttotal: 32.2ms\tremaining: 102ms\n",
      "24:\tlearn: 0.4343913\ttotal: 33.5ms\tremaining: 100ms\n",
      "25:\tlearn: 0.4326627\ttotal: 34.8ms\tremaining: 99ms\n",
      "26:\tlearn: 0.4288752\ttotal: 36ms\tremaining: 97.3ms\n",
      "27:\tlearn: 0.4274490\ttotal: 37.2ms\tremaining: 95.6ms\n",
      "28:\tlearn: 0.4253860\ttotal: 38.8ms\tremaining: 95ms\n",
      "29:\tlearn: 0.4227706\ttotal: 40.2ms\tremaining: 93.7ms\n",
      "30:\tlearn: 0.4213188\ttotal: 41.4ms\tremaining: 92.1ms\n",
      "31:\tlearn: 0.4196741\ttotal: 42.6ms\tremaining: 90.6ms\n",
      "32:\tlearn: 0.4180128\ttotal: 44ms\tremaining: 89.4ms\n",
      "33:\tlearn: 0.4170621\ttotal: 45.4ms\tremaining: 88.1ms\n",
      "34:\tlearn: 0.4156327\ttotal: 46.9ms\tremaining: 87.1ms\n",
      "35:\tlearn: 0.4145383\ttotal: 48.3ms\tremaining: 85.8ms\n",
      "36:\tlearn: 0.4126750\ttotal: 49.6ms\tremaining: 84.4ms\n",
      "37:\tlearn: 0.4111022\ttotal: 50.9ms\tremaining: 83.1ms\n",
      "38:\tlearn: 0.4098814\ttotal: 52.4ms\tremaining: 82ms\n",
      "39:\tlearn: 0.4086619\ttotal: 54ms\tremaining: 80.9ms\n",
      "40:\tlearn: 0.4077735\ttotal: 55.6ms\tremaining: 80.1ms\n",
      "41:\tlearn: 0.4066718\ttotal: 57.1ms\tremaining: 78.8ms\n",
      "42:\tlearn: 0.4055510\ttotal: 58.5ms\tremaining: 77.5ms\n",
      "43:\tlearn: 0.4043276\ttotal: 60ms\tremaining: 76.3ms\n",
      "44:\tlearn: 0.4034529\ttotal: 61.3ms\tremaining: 74.9ms\n",
      "45:\tlearn: 0.4018710\ttotal: 62.6ms\tremaining: 73.5ms\n",
      "46:\tlearn: 0.4010146\ttotal: 63.8ms\tremaining: 72ms\n",
      "47:\tlearn: 0.4004082\ttotal: 65ms\tremaining: 70.4ms\n",
      "48:\tlearn: 0.3999666\ttotal: 66.2ms\tremaining: 68.9ms\n",
      "49:\tlearn: 0.3991842\ttotal: 67.5ms\tremaining: 67.5ms\n",
      "50:\tlearn: 0.3983549\ttotal: 69ms\tremaining: 66.3ms\n",
      "51:\tlearn: 0.3978140\ttotal: 70.3ms\tremaining: 64.9ms\n",
      "52:\tlearn: 0.3971946\ttotal: 71.6ms\tremaining: 63.5ms\n",
      "53:\tlearn: 0.3962431\ttotal: 72.9ms\tremaining: 62.1ms\n",
      "54:\tlearn: 0.3959212\ttotal: 74.6ms\tremaining: 61ms\n",
      "55:\tlearn: 0.3950411\ttotal: 75.8ms\tremaining: 59.6ms\n",
      "56:\tlearn: 0.3937210\ttotal: 77.1ms\tremaining: 58.2ms\n",
      "57:\tlearn: 0.3931061\ttotal: 79.8ms\tremaining: 57.8ms\n",
      "58:\tlearn: 0.3919717\ttotal: 82.1ms\tremaining: 57ms\n",
      "59:\tlearn: 0.3913323\ttotal: 84ms\tremaining: 56ms\n",
      "60:\tlearn: 0.3901337\ttotal: 86.7ms\tremaining: 55.4ms\n",
      "61:\tlearn: 0.3896412\ttotal: 88.6ms\tremaining: 54.3ms\n",
      "62:\tlearn: 0.3890994\ttotal: 90.6ms\tremaining: 53.2ms\n",
      "63:\tlearn: 0.3881717\ttotal: 92.6ms\tremaining: 52.1ms\n",
      "64:\tlearn: 0.3875367\ttotal: 94.6ms\tremaining: 50.9ms\n",
      "65:\tlearn: 0.3867911\ttotal: 96.8ms\tremaining: 49.8ms\n",
      "66:\tlearn: 0.3861113\ttotal: 99.1ms\tremaining: 48.8ms\n",
      "67:\tlearn: 0.3855606\ttotal: 101ms\tremaining: 47.6ms\n",
      "68:\tlearn: 0.3847789\ttotal: 103ms\tremaining: 46.4ms\n",
      "69:\tlearn: 0.3838843\ttotal: 105ms\tremaining: 45.1ms\n",
      "70:\tlearn: 0.3826088\ttotal: 107ms\tremaining: 43.8ms\n",
      "71:\tlearn: 0.3818520\ttotal: 110ms\tremaining: 42.7ms\n",
      "72:\tlearn: 0.3811822\ttotal: 111ms\tremaining: 41.1ms\n",
      "73:\tlearn: 0.3803804\ttotal: 113ms\tremaining: 39.8ms\n",
      "74:\tlearn: 0.3789493\ttotal: 115ms\tremaining: 38.4ms\n",
      "75:\tlearn: 0.3786408\ttotal: 117ms\tremaining: 36.9ms\n",
      "76:\tlearn: 0.3783641\ttotal: 118ms\tremaining: 35.3ms\n",
      "77:\tlearn: 0.3777685\ttotal: 120ms\tremaining: 33.7ms\n",
      "78:\tlearn: 0.3769159\ttotal: 121ms\tremaining: 32.1ms\n",
      "79:\tlearn: 0.3761053\ttotal: 122ms\tremaining: 30.5ms\n",
      "80:\tlearn: 0.3756459\ttotal: 123ms\tremaining: 28.9ms\n",
      "81:\tlearn: 0.3750716\ttotal: 125ms\tremaining: 27.4ms\n",
      "82:\tlearn: 0.3744970\ttotal: 126ms\tremaining: 25.8ms\n",
      "83:\tlearn: 0.3734793\ttotal: 128ms\tremaining: 24.3ms\n",
      "84:\tlearn: 0.3730872\ttotal: 130ms\tremaining: 22.9ms\n",
      "85:\tlearn: 0.3725307\ttotal: 131ms\tremaining: 21.3ms\n",
      "86:\tlearn: 0.3720987\ttotal: 132ms\tremaining: 19.8ms\n",
      "87:\tlearn: 0.3714777\ttotal: 134ms\tremaining: 18.2ms\n",
      "88:\tlearn: 0.3709630\ttotal: 135ms\tremaining: 16.7ms\n",
      "89:\tlearn: 0.3704033\ttotal: 136ms\tremaining: 15.2ms\n",
      "90:\tlearn: 0.3692056\ttotal: 138ms\tremaining: 13.6ms\n",
      "91:\tlearn: 0.3684954\ttotal: 139ms\tremaining: 12.1ms\n",
      "92:\tlearn: 0.3677146\ttotal: 141ms\tremaining: 10.6ms\n",
      "93:\tlearn: 0.3671094\ttotal: 142ms\tremaining: 9.06ms\n",
      "94:\tlearn: 0.3667969\ttotal: 143ms\tremaining: 7.54ms\n",
      "95:\tlearn: 0.3663941\ttotal: 144ms\tremaining: 6.02ms\n",
      "96:\tlearn: 0.3661872\ttotal: 146ms\tremaining: 4.51ms\n",
      "97:\tlearn: 0.3654506\ttotal: 147ms\tremaining: 3ms\n",
      "98:\tlearn: 0.3650920\ttotal: 148ms\tremaining: 1.5ms\n",
      "99:\tlearn: 0.3642801\ttotal: 150ms\tremaining: 0us\n",
      "My accuracy: 0.8659863945578231, SK accuracy: 0.863265306122449\n",
      "XGBoost Accuracy: 0.8224489795918367\n",
      "LightGBM Accuracy: 0.8204081632653061\n",
      "CatBoost Accuracy: 0.8122448979591836\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing XGBoost, LightGBM and CatBoost\")\n",
    "# XGBoost\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "# LightGBM\n",
    "params = {'verbose': -1}\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(X_test)\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "\n",
    "# CatBoost\n",
    "cb_classifier = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3)\n",
    "cb_classifier.fit(X_train, y_train)\n",
    "y_pred_cb = cb_classifier.predict(X_test)\n",
    "accuracy_cb = accuracy_score(y_test, y_pred_cb)\n",
    "\n",
    "print(f\"My accuracy: {accuracy}, SK accuracy: {sk_acc}\")\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb}\")\n",
    "print(f\"LightGBM Accuracy: {accuracy_lgb}\")\n",
    "print(f\"CatBoost Accuracy: {accuracy_cb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
